Week 3:
	I revamped our database schema to better fit the data we had available, and implemented 
	methods to scrape the data and then write it to the db for 3 of the 5 tables, including
	the 2 most critical tables (Sources and Contaminants).
	
	Files worked on:
		/backend/vodabackend/vodaMainApp/models.py
		webscraper/findUtilitiesScraper.py
		webscraper/utilityInfoScraper.py
		webscraper/getContaminantsScraper.py
		webscraper/contaminantInfoScraper.py

Week 2:
	I found a found csv of all us zipcodes from us census bureau and changed scraper to pull from every zip codes. 
	This way we can collect info on even the smallest utilities. I also made a local postgres database to 
	model the one we will be hosting, and began setting up writes from the scraper to the database after 
	learning how to use both postgres and psycopg2. 
	
	Files worked on:
		webscraper/findUtilitiesScraper.py
		webscraper/utilityInfoScraper.py
		webscraper/requirements.txt
		
		Found zipcodes.txt


Week 1:
	created basic web scraper to collect data from https://www.ewg.org/tapwater/ . 
	The scraper is split into 3 files. 1 collects all states/territories which data 
	is available for, 1 collects the water utlity providers from each state, and 1 collects
	the preliminary batch of data from each utlity provider.
	
	Files worked on:
		webscraper/findStatesScraper.py
		webscraper/findUtilitiesScraper.py
		webscraper/utilityInfoScraper.py
		webscraper/requirements.txt
		
		These files were created by the above scripts:
		webscraper/resultFiles/FinalInfo.txt
		webscraper/resultFiles/AllEWGUtilities.txt
		webscraper/resultFiles/EWGStates.txt
