Week 4:
	I made corrections to the scheam from last week and spent time getting the scraper into compliance
	with our liner. I also implemented methods for scraping the data and writing it for the 
	source_levels table. During this process I uncovered a serious bug with the methods written
	last week to handle the contaminants data. I spent many hours trying to understand this
	bug and in the end realized that it had to do with the asynchronous nature of
	the requests Scrapy uses, and know how to fix the problem. I did not implement a fix this week 
	because the fix will require refactoring a large portion of the scraper code, and there was not 
	time to both diagnose and fix it this week. This will be done in the next week. 
	
	Files worked on:
		/backend/vodabackend/vodaMainApp/models.py
		webscraper/findUtilitiesScraper.py
		webscraper/utilityInfoScraper.py
		webscraper/getContaminantsScraper.py
		webscraper/contaminantInfoScraper.py

Week 3:
	I revamped our database schema to better fit the data we had available, and implemented 
	methods to scrape the data and then write it to the db for 3 of the 5 tables, including
	the 2 most critical tables (Sources and Contaminants).
	
	Files worked on:
		/backend/vodabackend/vodaMainApp/models.py
		webscraper/findUtilitiesScraper.py
		webscraper/utilityInfoScraper.py
		webscraper/getContaminantsScraper.py
		webscraper/contaminantInfoScraper.py

Week 2:
	I found a found csv of all us zipcodes from us census bureau and changed scraper to pull from every zip codes. 
	This way we can collect info on even the smallest utilities. I also made a local postgres database to 
	model the one we will be hosting, and began setting up writes from the scraper to the database after 
	learning how to use both postgres and psycopg2. 
	
	Files worked on:
		webscraper/findUtilitiesScraper.py
		webscraper/utilityInfoScraper.py
		webscraper/requirements.txt
		
		Found zipcodes.txt


Week 1:
	created basic web scraper to collect data from https://www.ewg.org/tapwater/ . 
	The scraper is split into 3 files. 1 collects all states/territories which data 
	is available for, 1 collects the water utlity providers from each state, and 1 collects
	the preliminary batch of data from each utlity provider.
	
	Files worked on:
		webscraper/findStatesScraper.py
		webscraper/findUtilitiesScraper.py
		webscraper/utilityInfoScraper.py
		webscraper/requirements.txt
		
		These files were created by the above scripts:
		webscraper/resultFiles/FinalInfo.txt
		webscraper/resultFiles/AllEWGUtilities.txt
		webscraper/resultFiles/EWGStates.txt
